{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import ConvLSTM2D\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 20 30 40]\n",
      " [20 30 40 50]\n",
      " [30 40 50 60]\n",
      " [40 50 60 70]\n",
      " [50 60 70 80]]\n",
      "\n",
      "\n",
      "[50 60 70 80 90]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "print(X)\n",
    "print('\\n')\n",
    "print(y)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[[10]\n",
      "    [20]]]\n",
      "\n",
      "\n",
      "  [[[30]\n",
      "    [40]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[20]\n",
      "    [30]]]\n",
      "\n",
      "\n",
      "  [[[40]\n",
      "    [50]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[30]\n",
      "    [40]]]\n",
      "\n",
      "\n",
      "  [[[50]\n",
      "    [60]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[40]\n",
      "    [50]]]\n",
      "\n",
      "\n",
      "  [[[60]\n",
      "    [70]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[50]\n",
      "    [60]]]\n",
      "\n",
      "\n",
      "  [[[70]\n",
      "    [80]]]]]\n"
     ]
    }
   ],
   "source": [
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "#model = Sequential()\n",
    "#model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "#model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "#model.add(TimeDistributed(Flatten()))\n",
    "#model.add(LSTM(50, activation='relu'))\n",
    "#model.add(Dense(1))\n",
    "#model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConvLSTM\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[10]\n",
      "  [20]\n",
      "  [30]]\n",
      "\n",
      " [[20]\n",
      "  [30]\n",
      "  [40]]\n",
      "\n",
      " [[30]\n",
      "  [40]\n",
      "  [50]]\n",
      "\n",
      " [[40]\n",
      "  [50]\n",
      "  [60]]\n",
      "\n",
      " [[50]\n",
      "  [60]\n",
      "  [70]]\n",
      "\n",
      " [[60]\n",
      "  [70]\n",
      "  [80]]]\n"
     ]
    }
   ],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "#n_features = 1\n",
    "#X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "#print(X)\n",
    "\n",
    "# BIDIRECTIONAL LSTM\n",
    "#model = Sequential()\n",
    "#model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "#model.add(Dense(1))\n",
    "#model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cf15a77f70>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[[60]\n",
      "    [70]]]\n",
      "\n",
      "\n",
      "  [[[80]\n",
      "    [90]]]]]\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = array([60, 70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CF18B88280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[103.82957]]\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
